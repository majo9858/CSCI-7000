{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c262549e-ba9e-4dc8-9f58-47a8976ff12d",
      "metadata": {
        "id": "c262549e-ba9e-4dc8-9f58-47a8976ff12d"
      },
      "source": [
        "# Dynamic Programming to Optimize Discounted Rewards\n",
        "\n",
        "Let's consider the weighted finite graph discussed in the lecture on 06/08 and proceed with modeling it. Our objective is to develop a range of algorithms that can efficiently compute the optimal value as well as an optimal policy for this graph.\n",
        "\n",
        "<center>\n",
        "    <img width=50% src=\"graph01.png\">\n",
        "</center>\n",
        "\n",
        "We will implement three algorithms:\n",
        "1. value iteration\n",
        "2. policy iteration\n",
        "3. linear programming\n",
        "\n",
        "Let's first start by importing numpy and pulp packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pulp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwdsPtKMGmcO",
        "outputId": "a2631752-f835-49d4-f3f3-c45de1997b4d"
      },
      "id": "YwdsPtKMGmcO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pulp\n",
            "  Downloading PuLP-2.7.0-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c426e8e9-18d7-4f6e-aa48-855deb52f26a",
      "metadata": {
        "id": "c426e8e9-18d7-4f6e-aa48-855deb52f26a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pulp import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c36e96-926b-471e-8870-d5a2e4487fda",
      "metadata": {
        "id": "d5c36e96-926b-471e-8870-d5a2e4487fda"
      },
      "source": [
        "In our model, we aim to keep it flexible enough to accommodate the modeling of Markov Decision Processes (MDPs). To achieve this, we represent transitions and rewards using three-dimensional matrices of size |S|x|A|x|S|. These matrices provide the transition probabilities and rewards associated with each transition (s, a, s')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c9166175-9786-4e69-ab07-1070ca55878c",
      "metadata": {
        "id": "c9166175-9786-4e69-ab07-1070ca55878c"
      },
      "outputs": [],
      "source": [
        "# A simple MDP\n",
        "\n",
        "states = ['s1', 's2']\n",
        "actions = ['a', 'b']\n",
        "\n",
        "# transitions[s][a][s'] = probability of the transition (s, a, s')\n",
        "transitions = np.array([\n",
        "    [[1.0, 0.0], [0.0, 1.0]],\n",
        "    [[0.0, 1.0], [1.0, 0.0]]\n",
        "])\n",
        "\n",
        "# rewards[s][a][s'] = reward of the transition (s, a, s')\n",
        "rewards = np.array([\n",
        "    [[2.0, 0.0], [0.0, 1.0]],\n",
        "    [[0.0, 5.0], [8.0, 0.0]],\n",
        "])\n",
        "\n",
        "discount = 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ba2ffc-330b-48ea-a0f1-72313673015d",
      "metadata": {
        "id": "18ba2ffc-330b-48ea-a0f1-72313673015d"
      },
      "source": [
        "## Value Iteration\n",
        "Our first algorithm is value iteration. It is based on Bellman operator defined from the Belmman optimality equations\n",
        "\n",
        "<center><img width=50% src=\"bellman.png\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "04c295c8-1dbe-41c5-8fe4-366e34367824",
      "metadata": {
        "id": "04c295c8-1dbe-41c5-8fe4-366e34367824",
        "outputId": "d51f8fc1-8f53-4edf-9381-65d73d676173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration:\n",
            "[20.99999997 24.99999997]\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(epsilon=1e-8, max_iterations=1000):\n",
        "    num_states = len(states)\n",
        "    num_actions = len(actions)\n",
        "    V = np.zeros(num_states)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        V_prev = np.copy(V)\n",
        "        for s in range(num_states):\n",
        "            Q = np.zeros(num_actions)\n",
        "            for a in range(num_actions):\n",
        "                for next_state in range(num_states):\n",
        "                    reward = rewards[s][a][next_state]\n",
        "                    prob = transitions[s][a][next_state]\n",
        "                    Q[a] += prob * (reward + discount * V_prev[next_state])\n",
        "            V[s] = np.max(Q)\n",
        "\n",
        "        if np.max(np.abs(V - V_prev)) < epsilon:\n",
        "            break\n",
        "\n",
        "    return V\n",
        "\n",
        "# Value Iteration\n",
        "print(\"Value Iteration:\")\n",
        "v_values = value_iteration()\n",
        "print(v_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f01979fb-3bb6-42e5-8146-3fd2387c82de",
      "metadata": {
        "id": "f01979fb-3bb6-42e5-8146-3fd2387c82de"
      },
      "outputs": [],
      "source": [
        "def compute_value(policy, epsilon=1e-8, max_iterations=1000):\n",
        "    num_states = len(states)\n",
        "    num_actions = len(actions)\n",
        "    V = np.zeros(num_states)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        V_prev = np.copy(V)\n",
        "\n",
        "        for s in range(num_states):\n",
        "            a = policy[s]\n",
        "            Q = np.zeros(num_actions)\n",
        "\n",
        "            for next_state in range(num_states):\n",
        "                Q[a] += rewards[s][a][next_state] + discount * transitions[s][a][next_state] * V_prev[next_state]      # Q(2): Update this Q[a] properly\n",
        "\n",
        "            V[s] = Q[a]\n",
        "\n",
        "        if np.max(np.abs(V - V_prev)) < epsilon:\n",
        "            break\n",
        "\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4103a997-222f-4404-a5fe-1fcaa6ab9633",
      "metadata": {
        "id": "4103a997-222f-4404-a5fe-1fcaa6ab9633"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(max_iterations=1000):\n",
        "    num_states = len(states)\n",
        "    num_actions = len(actions)\n",
        "    V = np.zeros(num_states)\n",
        "    policy = np.zeros(num_states, dtype=int)\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        V = compute_value(policy)\n",
        "        policy_stable = True\n",
        "\n",
        "        for s in range(num_states):\n",
        "            old_action = policy[s]\n",
        "            Q = np.zeros(num_actions)\n",
        "\n",
        "            for a in range(num_actions):\n",
        "                for next_state in range(num_states):\n",
        "                    Q[a] = rewards[s][a][next_state] + discount * transitions[s][a][next_state] * V[next_state] # Q(3): Update this value properly\n",
        "\n",
        "            if (Q[policy[s]] != np.argmax(Q)):\n",
        "                policy[s] = np.argmax(Q)\n",
        "\n",
        "            if old_action != policy[s]:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return V, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b4f6ab40-db49-476f-8f07-3c66e1666c48",
      "metadata": {
        "id": "b4f6ab40-db49-476f-8f07-3c66e1666c48",
        "outputId": "48a94b7c-b76c-4f8e-991a-e0c74466824d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Policy Iteration:\n",
            "Values:\n",
            "[20.99999996 24.99999996]\n",
            "Policy:\n",
            "[1 0]\n"
          ]
        }
      ],
      "source": [
        "# Policy Iteration\n",
        "print(\"\\nPolicy Iteration:\")\n",
        "v_values, policy = policy_iteration()\n",
        "print(\"Values:\")\n",
        "print(v_values)\n",
        "print(\"Policy:\")\n",
        "print(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8fbf7529-a5ab-40ff-a412-26c4f53e6df3",
      "metadata": {
        "id": "8fbf7529-a5ab-40ff-a412-26c4f53e6df3"
      },
      "outputs": [],
      "source": [
        "    def linear_programming():\n",
        "        num_states = len(states)\n",
        "        num_actions = len(actions)\n",
        "\n",
        "        # Create an LP Minimization problem\n",
        "        lp_prob = LpProblem(\"LinearProgramming\", LpMinimize)\n",
        "\n",
        "        # Create problem Variables\n",
        "        V = LpVariable.dicts(\"V\", range(num_states), lowBound=0)\n",
        "\n",
        "        # Objective Function \"Minimize \\Sigma_{i=0}^{n-1} V[i]\"\n",
        "        lp_prob += lpSum(V)\n",
        "\n",
        "        # Constraints:\n",
        "        # For every transition (s, a, s') we have V[s] >= r(s, a, s') + discount * V[s']\n",
        "        for s in range(num_states):\n",
        "            for a in range(num_actions):\n",
        "                action_sum = 0\n",
        "                for next_state in range(num_states):\n",
        "                    prob_val = transitions[s, a, next_state]\n",
        "                    action_sum += prob_val * (rewards[s, a, next_state] + discount * V[next_state])\n",
        "\n",
        "                lp_prob += V[s] >= action_sum\n",
        "\n",
        "\n",
        "        lp_prob.solve(PULP_CBC_CMD(msg=0))\n",
        "\n",
        "        v_values = np.array([value(V[i]) for i in range(num_states)])\n",
        "\n",
        "        return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2cff6f51-2591-4fb1-b174-cfe81b1fec8d",
      "metadata": {
        "id": "2cff6f51-2591-4fb1-b174-cfe81b1fec8d",
        "outputId": "52c3970f-f2aa-4182-d4f4-9796011ab363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Linear Programming:\n",
            "[21. 25.]\n"
          ]
        }
      ],
      "source": [
        "# Linear Programming\n",
        "print(\"\\nLinear Programming:\")\n",
        "v_values = linear_programming()\n",
        "print(v_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ff390851-b9af-42c7-9e8e-37950355e572",
      "metadata": {
        "id": "ff390851-b9af-42c7-9e8e-37950355e572"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}